{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d41fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- Cấu hình Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ab646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    # Đường dẫn đến ChromeDriver trong thư mục chrome-linux64\n",
    "    chrome_driver_path = \"/home/minh/codeproject/epl_crawl/chrome-linux64/chrome\"\n",
    "\n",
    "    try:\n",
    "        service = Service(chrome_driver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        logging.info(\"WebDriver đã được khởi tạo.\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Lỗi khởi tạo WebDriver: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4fb1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hàm lấy link Match Report từ trang lịch thi đấu --- (Giữ nguyên bản Selenium)\n",
    "def get_match_report_links_selenium(driver, schedule_url, base_url=\"https://fbref.com\"):\n",
    "    links = []\n",
    "    if not driver: return links\n",
    "    try:\n",
    "        logging.info(f\"Đang lấy link từ: {schedule_url} bằng Selenium\")\n",
    "        driver.get(schedule_url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table[id^='sched_'] , table.stats_table\")))\n",
    "        time.sleep(1)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        table = soup.find('table', id=re.compile(r'^sched_.*'))\n",
    "        if not table: table = soup.find('table', class_='stats_table')\n",
    "        if not table: logging.error(f\"Không tìm thấy bảng lịch thi đấu: {schedule_url}\"); return []\n",
    "\n",
    "        match_report_cells = table.find_all('td', {'data-stat': 'match_report'})\n",
    "        for cell in match_report_cells:\n",
    "            link_tag = cell.find('a', href=True)\n",
    "            if link_tag:\n",
    "                relative_url = link_tag['href']\n",
    "                if '/en/matches/' in relative_url:\n",
    "                    full_url = urljoin(base_url, relative_url)\n",
    "                    links.append(full_url)\n",
    "        logging.info(f\"Tìm thấy {len(links)} link Match Report.\")\n",
    "        return list(dict.fromkeys(links)) # Loại bỏ trùng lặp\n",
    "\n",
    "    except Exception as e: logging.error(f\"Lỗi lấy link từ {schedule_url}: {e}\", exc_info=False); return []\n",
    "\n",
    "# --- Hàm lấy League và Season --- (Giữ nguyên)\n",
    "def get_league_season_info(driver, schedule_url):\n",
    "    league_name, season_str = None, None\n",
    "    if not driver: return league_name, season_str\n",
    "    try:\n",
    "        logging.info(f\"Đang lấy thông tin League/Season từ: {schedule_url}\")\n",
    "        driver.get(schedule_url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"h1\")))\n",
    "        time.sleep(1)\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        h1_tag = soup.find('h1')\n",
    "        if h1_tag:\n",
    "            season_match = re.search(r'(\\d{4}-\\d{4}|\\d{4})', h1_tag.text)\n",
    "            if season_match: season_str = season_match.group(1)\n",
    "            league_name_raw = h1_tag.text\n",
    "            if season_str: league_name_raw = league_name_raw.replace(season_str, '')\n",
    "            league_name_raw = re.sub(r'Scores?.+Fixtures?', '', league_name_raw, flags=re.IGNORECASE).strip()\n",
    "            league_name = league_name_raw\n",
    "        logging.info(f\"League: {league_name}, Season: {season_str}\")\n",
    "        return league_name, season_str\n",
    "    except Exception as e: logging.error(f\"Lỗi lấy league/season từ {schedule_url}: {e}\", exc_info=False); return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b661b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hàm phụ: Lấy đội hình, đội hình dự bị và sơ đồ chiến thuật ---\n",
    "def get_lineup_formation_bench(soup, team_id_char, match_url):\n",
    "    starting_lineup = []; bench = []; formation = None\n",
    "    try:\n",
    "        lineup_div = soup.find('div', class_='lineup', id=team_id_char)\n",
    "        if lineup_div:\n",
    "            lineup_table = lineup_div.find('table')\n",
    "            if lineup_table:\n",
    "                header_th = lineup_table.find('th', colspan=\"2\")\n",
    "                if header_th:\n",
    "                    header_text = header_th.get_text(strip=True); formation_match = re.search(r'\\(([\\d\\-]+)\\)', header_text)\n",
    "                    if formation_match: formation = formation_match.group(1)\n",
    "\n",
    "                bench_header_row_found = None\n",
    "                bench_header_th = lineup_table.find('th', string='Bench')\n",
    "                if bench_header_th: bench_header_row_found = bench_header_th.find_parent('tr')\n",
    "\n",
    "                tbody = lineup_table.find('tbody')\n",
    "                if tbody:\n",
    "                    player_rows = tbody.find_all('tr')\n",
    "                    bench_started = False; starter_count = 0\n",
    "                    for row in player_rows:\n",
    "                        if row == bench_header_row_found: bench_started = True; continue\n",
    "                        if row.find('th') and not row.find('a', href=lambda href: href and '/players/' in href): continue\n",
    "                        player_link = row.find('a', href=lambda href: href and '/players/' in href)\n",
    "                        if player_link:\n",
    "                            player_name = player_link.text.strip()\n",
    "                            if player_name:\n",
    "                                if not bench_started and starter_count < 11: starting_lineup.append(player_name); starter_count += 1\n",
    "                                elif bench_started or starter_count >= 11: bench.append(player_name)\n",
    "                    if not bench_header_row_found and len(starting_lineup) == 11:\n",
    "                        all_player_links = lineup_table.find_all('a', href=lambda href: href and '/players/' in href)\n",
    "                        all_player_names = [link.text.strip() for link in all_player_links if link.text.strip()]\n",
    "                        if len(all_player_names) > 11: bench = all_player_names[11:]\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"[{match_url}] Lỗi khi lấy lineup/bench team '{team_id_char}': {e}\", exc_info=False) # Giảm độ chi tiết log lỗi\n",
    "\n",
    "    lineup_output = [formation] + starting_lineup if formation else starting_lineup\n",
    "    # Đảm bảo lineup_output là một list duy nhất để ghi vào CSV\n",
    "    lineup_str = \", \".join(filter(None, lineup_output)) # Chuyển list thành string\n",
    "    bench_str = \", \".join(filter(None, bench)) # Chuyển list thành string\n",
    "\n",
    "    # Trả về dạng string để ghi CSV dễ hơn, hoặc giữ nguyên list nếu muốn xử lý sau\n",
    "    return lineup_str, bench_str\n",
    "    # return lineup_output, bench # Nếu muốn trả về list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c92cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hàm phụ: Lấy tổng Shots, SoT, Cards từ tfoot bảng Player Stats ---\n",
    "def get_totals_from_player_stats_tfoot(soup, specific_table_id, match_url):\n",
    "    totals = {'shots': 0, 'shots_on_target': 0, 'yellow_cards': 0, 'red_cards': 0}\n",
    "    if not specific_table_id: return totals\n",
    "    try:\n",
    "        table = soup.find('table', id=specific_table_id)\n",
    "        if not table: logging.warning(f\"[{match_url}] Không tìm thấy bảng player stats ID: {specific_table_id}\"); return totals\n",
    "        thead = table.find('thead'); tfoot = table.find('tfoot')\n",
    "        if thead and tfoot:\n",
    "            header_row = thead.find_all('tr')[-1]; headers_th = header_row.find_all('th')\n",
    "            headers = [th.get('data-stat') for th in headers_th]\n",
    "            total_row = tfoot.find('tr')\n",
    "            if total_row:\n",
    "                cells = total_row.find_all(['th', 'td'])\n",
    "                stat_map = {'shots': 'shots', 'shots_on_target': 'shots_on_target', 'cards_yellow': 'yellow_cards', 'cards_red': 'red_cards'}\n",
    "                for data_stat_name, target_key in stat_map.items():\n",
    "                    try:\n",
    "                        if data_stat_name in headers:\n",
    "                            col_index = headers.index(data_stat_name)\n",
    "                            if col_index < len(cells):\n",
    "                                value_str = cells[col_index].text.strip()\n",
    "                                totals[target_key] = int(value_str) if value_str.isdigit() else 0\n",
    "                            else: logging.warning(f\"[{match_url}] Index cột '{data_stat_name}' không hợp lệ tfoot {specific_table_id}.\"); totals[target_key] = 0\n",
    "                        else: logging.warning(f\"[{match_url}] Không tìm thấy header '{data_stat_name}' trong {specific_table_id}.\"); totals[target_key] = 0\n",
    "                    except ValueError: logging.warning(f\"[{match_url}] Giá trị không hợp lệ tfoot {specific_table_id}: '{cells[col_index].text.strip()}'. Gán 0.\"); totals[target_key] = 0\n",
    "                    except Exception as e_inner: logging.warning(f\"[{match_url}] Lỗi lấy tfoot {target_key} {specific_table_id}: {e_inner}\"); totals[target_key] = 0\n",
    "            else: logging.warning(f\"[{match_url}] Không thấy hàng trong tfoot {specific_table_id}\")\n",
    "        else: logging.warning(f\"[{match_url}] Không thấy thead/tfoot trong {specific_table_id}\")\n",
    "    except Exception as e: logging.error(f\"[{match_url}] Lỗi nghiêm trọng xử lý tfoot {specific_table_id}: {e}\", exc_info=False)\n",
    "    return totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034c593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hàm phụ: Lấy Saves từ bảng Goalkeeper Stats ---\n",
    "def get_saves_from_keeper_stats(soup, specific_div_id, match_url):\n",
    "    saves = 0\n",
    "    if not specific_div_id: return saves\n",
    "    try:\n",
    "        target_div = soup.find('div', id=specific_div_id)\n",
    "        if not target_div: logging.warning(f\"[{match_url}] Không tìm thấy div keeper stats: {specific_div_id}\"); return saves\n",
    "        keeper_table = target_div.find('table', id=re.compile(r'^keeper_stats_'))\n",
    "        if keeper_table:\n",
    "            tbody = keeper_table.find('tbody')\n",
    "            if tbody:\n",
    "                keeper_rows = tbody.find_all('tr'); total_saves = 0\n",
    "                for keeper_row in keeper_rows:\n",
    "                    saves_cell = keeper_row.find('td', {'data-stat': 'gk_saves'})\n",
    "                    if saves_cell:\n",
    "                        try: saves_val = saves_cell.text.strip(); total_saves += int(saves_val) if saves_val.isdigit() else 0\n",
    "                        except ValueError: logging.warning(f\"[{match_url}] Giá trị saves không hợp lệ {specific_div_id}: '{saves_val}'.\")\n",
    "                saves = total_saves\n",
    "    except Exception as e: logging.warning(f\"[{match_url}] Lỗi lấy saves từ {specific_div_id}: {e}\")\n",
    "    return saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3587887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hàm chính: Crawl dữ liệu chi tiết từ một trang Match Report ---\n",
    "def scrape_match_data_selenium(driver, match_url, league_name, season_str, source_url):\n",
    "    \"\"\"Crawl dữ liệu chi tiết từ một link Match Report dùng Selenium.\"\"\"\n",
    "    match_data = {'match_report_url': match_url, 'league': league_name, 'season': season_str, 'source': source_url}\n",
    "    if not driver: return None\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Đang crawl: {match_url}\")\n",
    "        driver.get(match_url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.scorebox\")))\n",
    "        time.sleep(1) # Ngắn hơn để tăng tốc\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # -- Round (Week) --\n",
    "        match_data['round'] = 0 # Mặc định 0\n",
    "        try:\n",
    "            header_info_div = soup.find('h1').find_next_sibling('div')\n",
    "            if header_info_div:\n",
    "                week_text = header_info_div.text\n",
    "                week_match = re.search(r'\\((?:Matchweek|Round)\\s+(\\d+)\\)', week_text, re.IGNORECASE)\n",
    "                if week_match: match_data['round'] = int(week_match.group(1))\n",
    "        except Exception: logging.warning(f\"[{match_url}] Không tìm thấy round.\")\n",
    "\n",
    "        # -- Scorebox Data (Teams, Scores, Date, Venue) --\n",
    "        match_data['home'] = \"\"; match_data['away'] = \"\"; match_data['home_score'] = 0; match_data['away_score'] = 0\n",
    "        match_data['date'] = \"\"; match_data['venue'] = \"\"\n",
    "        try:\n",
    "            scorebox = soup.find('div', class_='scorebox')\n",
    "            if scorebox:\n",
    "                teams_divs = scorebox.find_all('div', recursive=False)[:2]\n",
    "                if len(teams_divs) == 2:\n",
    "                    home_a = teams_divs[0].select_one('strong > a'); away_a = teams_divs[1].select_one('strong > a')\n",
    "                    if home_a: match_data['home'] = home_a.text.strip()\n",
    "                    if away_a: match_data['away'] = away_a.text.strip()\n",
    "                    home_score_div = teams_divs[0].select_one('div.scores > div.score'); away_score_div = teams_divs[1].select_one('div.scores > div.score')\n",
    "                    if home_score_div:\n",
    "                        try: match_data['home_score'] = int(home_score_div.text.strip())\n",
    "                        except ValueError: pass # Bỏ qua nếu score không phải số\n",
    "                    if away_score_div:\n",
    "                        try: match_data['away_score'] = int(away_score_div.text.strip())\n",
    "                        except ValueError: pass\n",
    "                scorebox_meta = scorebox.find('div', class_='scorebox_meta')\n",
    "                if scorebox_meta:\n",
    "                    date_tag = scorebox_meta.find('strong'); date_link = date_tag.find('a') if date_tag else None\n",
    "                    if date_link : match_data['date'] = date_link.text.strip()\n",
    "                    elif date_tag: match_data['date'] = date_tag.text.split(',')[0].strip()\n",
    "                    venue_strong = scorebox_meta.find('strong', string='Venue')\n",
    "                    if venue_strong:\n",
    "                        venue_parent = venue_strong.find_parent('div')\n",
    "                        if venue_parent:\n",
    "                            venue_small = venue_parent.find('small', string=lambda t: t and t.strip() != 'Venue')\n",
    "                            if venue_small: match_data['venue'] = venue_small.text.strip()\n",
    "                            else:\n",
    "                                next_sibling = venue_strong.next_sibling\n",
    "                                if next_sibling and isinstance(next_sibling, str):\n",
    "                                    venue_cleaned = next_sibling.strip(':').strip()\n",
    "                                    if venue_cleaned: match_data['venue'] = venue_cleaned\n",
    "        except Exception as e: logging.warning(f\"[{match_url}] Lỗi scorebox: {e}\", exc_info=False)\n",
    "\n",
    "        # -- Lineup, Formation, Bench --\n",
    "        match_data['home_lineup'], match_data['home_missing'] = get_lineup_formation_bench(soup, 'a', match_url)\n",
    "        match_data['away_lineup'], match_data['away_missing'] = get_lineup_formation_bench(soup, 'b', match_url)\n",
    "\n",
    "        # -- Possession & Pass Accuracy --\n",
    "        home_stats = {}; away_stats = {}\n",
    "        try:\n",
    "            team_stats_div = soup.find('div', id='team_stats')\n",
    "            if team_stats_div:\n",
    "                for header_text, key in {'Possession': 'Possession', 'Passing Accuracy': 'Passing Accuracy'}.items():\n",
    "                    header_th = team_stats_div.find('th', string=header_text)\n",
    "                    if header_th:\n",
    "                        data_row = header_th.find_parent('tr').find_next_sibling('tr')\n",
    "                        if data_row:\n",
    "                            values_td = data_row.find_all('td');\n",
    "                            if len(values_td) == 2:\n",
    "                                try:\n",
    "                                    home_text_raw = values_td[0].get_text(strip=True); away_text_raw = values_td[1].get_text(strip=True)\n",
    "                                    home_match = re.search(r'(\\d{1,3}(?:\\.\\d+)?%)', home_text_raw); away_match = re.search(r'(\\d{1,3}(?:\\.\\d+)?%)', away_text_raw)\n",
    "                                    if home_match: home_stats[key] = home_match.group(1)\n",
    "                                    if away_match: away_stats[key] = away_match.group(1)\n",
    "                                except Exception: pass\n",
    "        except Exception: pass # Bỏ qua nếu lỗi\n",
    "        match_data['home_possession'] = home_stats.get('Possession', '')\n",
    "        match_data['away_possession'] = away_stats.get('Possession', '')\n",
    "        match_data['home_pass_completion'] = home_stats.get('Passing Accuracy', '')\n",
    "        match_data['away_pass_completion'] = away_stats.get('Passing Accuracy', '')\n",
    "\n",
    "        # -- Fouls & Corners --\n",
    "        home_extra = {}; away_extra = {}\n",
    "        try:\n",
    "            team_stats_extra_div = soup.find('div', id='team_stats_extra')\n",
    "            if team_stats_extra_div:\n",
    "                stat_rows = team_stats_extra_div.find_all('div', recursive=False)\n",
    "                for row_div in stat_rows:\n",
    "                    inner_divs = row_div.find_all('div')\n",
    "                    if len(inner_divs) == 3:\n",
    "                        stat_name = inner_divs[1].text.strip()\n",
    "                        if stat_name == \"Fouls\" or stat_name == \"Corners\":\n",
    "                            try:\n",
    "                                home_val_str = inner_divs[0].text.strip(); away_val_str = inner_divs[2].text.strip()\n",
    "                                home_extra[stat_name] = int(home_val_str) if home_val_str.isdigit() else 0\n",
    "                                away_extra[stat_name] = int(away_val_str) if away_val_str.isdigit() else 0\n",
    "                            except Exception: pass\n",
    "        except Exception: pass\n",
    "        match_data['home_fouls'] = home_extra.get('Fouls', 0); match_data['away_fouls'] = away_extra.get('Fouls', 0)\n",
    "        match_data['home_corners'] = home_extra.get('Corners', 0); match_data['away_corners'] = away_extra.get('Corners', 0)\n",
    "\n",
    "        # -- Shots, SoT, Cards --\n",
    "        home_table_id = None; away_table_id = None\n",
    "        player_stat_tables = soup.find_all('table', id=re.compile(r'^stats_.*_summary'))\n",
    "        if len(player_stat_tables) >= 1: home_table_id = player_stat_tables[0].get('id')\n",
    "        if len(player_stat_tables) >= 2: away_table_id = player_stat_tables[1].get('id')\n",
    "        home_totals = get_totals_from_player_stats_tfoot(soup, home_table_id, match_url)\n",
    "        away_totals = get_totals_from_player_stats_tfoot(soup, away_table_id, match_url)\n",
    "        match_data['home_shots'] = home_totals.get('shots', 0); match_data['away_shots'] = away_totals.get('shots', 0)\n",
    "        match_data['home_shots_on_target'] = home_totals.get('shots_on_target', 0); match_data['away_shots_on_target'] = away_totals.get('shots_on_target', 0)\n",
    "        match_data['home_yellow_cards'] = home_totals.get('yellow_cards', 0); match_data['away_yellow_cards'] = away_totals.get('yellow_cards', 0)\n",
    "        match_data['home_red_cards'] = home_totals.get('red_cards', 0); match_data['away_red_cards'] = away_totals.get('red_cards', 0)\n",
    "\n",
    "        # -- Saves --\n",
    "        home_keeper_div_id = None; away_keeper_div_id = None\n",
    "        keeper_stat_divs = soup.find_all('div', id=re.compile(r'^all_keeper_stats_'))\n",
    "        if len(keeper_stat_divs) >= 1: home_keeper_div_id = keeper_stat_divs[0]['id']\n",
    "        if len(keeper_stat_divs) >= 2: away_keeper_div_id = keeper_stat_divs[1]['id']\n",
    "        match_data['home_saves'] = get_saves_from_keeper_stats(soup, home_keeper_div_id, match_url)\n",
    "        match_data['away_saves'] = get_saves_from_keeper_stats(soup, away_keeper_div_id, match_url)\n",
    "\n",
    "        # --- Final Data Dict ---\n",
    "        default_numeric = 0; default_string = \"\"; default_list_str = \"\" # Thay default_list bằng default_list_str\n",
    "        final_data_template = {\n",
    "            'date': default_string, 'home': default_string, 'away': default_string, 'home_score': default_numeric, 'away_score': default_numeric,\n",
    "            'league': default_string, 'season': default_string, 'source': default_string, 'match_report_url': default_string,\n",
    "            'round': default_numeric, 'venue': default_string,\n",
    "            'home_lineup': default_list_str, 'away_lineup': default_list_str, 'home_missing': default_list_str, 'away_missing': default_list_str, # Sử dụng default_list_str\n",
    "            'home_possession': default_string, 'away_possession': default_string, 'home_shots': default_numeric, 'away_shots': default_numeric,\n",
    "            'home_shots_on_target': default_numeric, 'away_shots_on_target': default_numeric, 'home_pass_completion': default_string, 'away_pass_completion': default_string,\n",
    "            'home_red_cards': default_numeric, 'away_red_cards': default_numeric, 'home_yellow_cards': default_numeric, 'away_yellow_cards': default_numeric,\n",
    "            'home_saves': default_numeric, 'away_saves': default_numeric, 'home_fouls': default_numeric, 'away_fouls': default_numeric,\n",
    "            'home_corners': default_numeric, 'away_corners': default_numeric\n",
    "        }\n",
    "        final_data = {key: match_data.get(key, default_value) for key, default_value in final_data_template.items()}\n",
    "\n",
    "        # Kiểm tra thiếu dữ liệu (log ít hơn)\n",
    "        # for key, value in final_data.items():\n",
    "        #     if value == final_data_template[key] and value is not 0 and value != \"\" and value != []:\n",
    "        #         logging.debug(f\"[{match_url}] Dữ liệu mặc định cho: {key}\")\n",
    "\n",
    "        return final_data\n",
    "\n",
    "    except TimeoutException: logging.error(f\"Timeout khi tải trang: {match_url}\"); return None\n",
    "    except WebDriverException as e: logging.error(f\"Lỗi WebDriver: {match_url}: {e}\"); return None\n",
    "    except Exception as e: logging.error(f\"Lỗi không xác định: {match_url}: {e}\", exc_info=False); return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192ac5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 13:56:47,761 - ERROR - Lỗi khởi tạo WebDriver: Message: Can not connect to the Service /home/minh/codeproject/epl_crawl/chrome-linux64/chrome\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Main --- (Giữ nguyên)\n",
    "if __name__ == \"__main__\":\n",
    "    schedule_page_url = \"https://fbref.com/en/comps/9/2014-2015/schedule/2014-2015-Premier-League-Scores-and-Fixtures\"\n",
    "    output_csv_file = \"fbref_premier_league_2014_2015_v18.csv\"\n",
    "    max_retries = 1; retry_delay = 10\n",
    "\n",
    "    driver = init_driver()\n",
    "\n",
    "    if driver:\n",
    "        league_name, season_str = get_league_season_info(driver, schedule_page_url)\n",
    "        if not league_name or not season_str:\n",
    "            logging.error(\"Không lấy được League/Season. Dừng.\")\n",
    "        else:\n",
    "            match_links = get_match_report_links_selenium(driver, schedule_page_url)\n",
    "            if not match_links: logging.error(\"Không lấy được link. Dừng.\")\n",
    "            else:\n",
    "                all_matches_data = []\n",
    "                processed_links_count = 0\n",
    "                for i, link in enumerate(match_links):\n",
    "                    attempt = 0; data = None\n",
    "                    while attempt <= max_retries:\n",
    "                        data = scrape_match_data_selenium(driver, link, league_name, season_str, schedule_page_url)\n",
    "                        if data:\n",
    "                            all_matches_data.append(data)\n",
    "                            processed_links_count += 1\n",
    "                            logging.info(f\"Đã crawl thành công {processed_links_count}/{len(match_links)} links.\")\n",
    "                            break\n",
    "                        else:\n",
    "                            attempt += 1\n",
    "                            logging.warning(f\"[{link}] Lần thử {attempt}/{max_retries + 1} lỗi.\")\n",
    "                            if attempt <= max_retries: time.sleep(retry_delay)\n",
    "                            else: logging.error(f\"[{link}] Bỏ qua link sau {max_retries} lần thử lại.\")\n",
    "                    time.sleep(2) # Tạm dừng giữa các link chính"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62f6ccdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_matches_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mall_matches_data\u001b[49m:\n\u001b[1;32m      2\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_matches_data)\n\u001b[1;32m      3\u001b[0m     columns_order \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maway\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maway_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleague\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mround\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvenue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_lineup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maway_lineup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_missing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maway_missing\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_corners\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maway_corners\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch_report_url\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m     ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_matches_data' is not defined"
     ]
    }
   ],
   "source": [
    "if all_matches_data:\n",
    "    df = pd.DataFrame(all_matches_data)\n",
    "    columns_order = [\n",
    "        'date', 'home', 'away', 'home_score', 'away_score', 'league', 'season',\n",
    "        'round', 'venue', 'home_lineup', 'away_lineup', 'home_missing', 'away_missing',\n",
    "        'home_possession', 'away_possession', 'home_shots', 'away_shots',\n",
    "        'home_shots_on_target', 'away_shots_on_target', 'home_pass_completion', 'away_pass_completion',\n",
    "        'home_red_cards', 'away_red_cards', 'home_yellow_cards', 'away_yellow_cards',\n",
    "        'home_saves', 'away_saves', 'home_fouls', 'away_fouls',\n",
    "        'home_corners', 'away_corners', 'match_report_url', 'source'\n",
    "    ]\n",
    "    columns_to_write = [col for col in columns_order if col in df.columns]\n",
    "    df = df[columns_to_write]\n",
    "    try:\n",
    "        df.to_csv(output_csv_file, index=False, encoding='utf-8-sig')\n",
    "        logging.info(f\"Đã lưu vào: {output_csv_file}\")\n",
    "    except Exception as e: logging.error(f\"Lỗi ghi CSV: {e}\")\n",
    "else:\n",
    "    logging.info(\"Không có dữ liệu để ghi.\")\n",
    "driver.quit()\n",
    "logging.info(\"WebDriver đã đóng.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
